%% %%=================================================================
%% %% <UTF-8>
%% %% 北航学位论文模板使用样例
%% %% 请将以下文件与此LaTeX文件放在同一目录中.
%% %%-----------
%% %% buaa.cls                  : LaTeX宏模板文件
%% %% bst/GBT7714-2005.bst      : 国标参考文献BibTeX样式文件2005(https://github.com/Haixing-Hu/GBT7714-2005-BibTeX-Style)
%% %% bst/GBT7714-2015.bst      : 国标参考文献BibTeX样式文件2015(https://github.com/zepinglee/gbt7714-bibtex-style)
%% %% pic/logo-buaa.eps         : 论文封皮北航字样
%% %% pic/head-doctor.eps       : 论文封皮学术博士学位论文标题(华文行楷字体替代解决方案)
%% %% pic/head-prodoctor.eps    : 论文封皮专业博士学位论文标题(华文行楷字体替代解决方案)
%% %% pic/head-master.eps       : 论文封皮学术硕士学位论文标题(华文行楷字体替代解决方案)
%% %% pic/head-professional.eps : 论文封皮专业硕士学位论文标题(华文行楷字体替代解决方案)
%% %% tex/*.tex                 : 本模板样例中的独立章节
%% %%-----------
%% %% 请统一使用UTF-8编码.
%% %%=================================================================

%=================================================================
% buaa基于ctexbook模板
% 论文样式参考自《研究生手册--二〇二〇年七月》
%======================
% 模板导入:
% \documentclass[thesis,permission,printtype,ostype,<ctexbookoptions>]{buaa}
%======================
% 模板选项:
%======================
% I.论文类型(thesis)
%--------------------
% a.学术硕士论文（master）[缺省值]
% b.专业硕士论文（professional）
% c.学术博士论文（doctor）
% d.专业博士论文（prodoctor）
%--------------------
% II.密级(permission)
%--------------------
% a.公开（public）[缺省值]
% b.内部（privacy）
% c.秘密（secret=secret3）
% c.1.秘密3年（secret3）
% c.2.秘密5年（secret5）
% c.3.秘密10年（secret10）
% c.4.秘密永久（secret*）
% d.机密（classified=classified5）
% d.1.机密3年（classified3）
% d.2.机密5年（classified5）
% d.3.机密10年（classified10）
% d.4.机密永久（classified*）
% e.绝密（topsecret=topsecret10）
% e.1.绝密3年（topsecret3）
% e.2.绝密5年（topsecret5）
% e.3.绝密10年（topsecret10）
% e.4.绝密永久（topsecret*）
%--------------------
% III.打印设置(printtype)
%--------------------
% a.单面打印（oneside）[缺省值]
% b.双面打印（twoside）
%--------------------
% IV.系统类型(ostype)
%--------------------
% a.win（oneside）[缺省值]
% b.linux (linux)
% c.mac (mac)
%--------------------
% V.ctexbook设置选项(<ctexbookoptions>)
%--------------------
% ...
%======================
% 其他说明:
% 1. Mac系统请使用mac选项，并使用XeLaTeX编译。
% 2. 可加入额外ctexbook文档类的选项，其将会被传递给ctexbook。
%    例如：\documentclass[fontset=founder]{buaa}
% 3. CTeX在Linux下默认使用Fandol字体，为避免某些生僻字无法显示，在系统已安装方正
%    字体的前提下可通过fontset=founder选项常用方正字体。
%=================================================================

% !TeX program = xelatex

\documentclass[master,oneside,max]{buaa}

%=================================================================
% 开启/关闭引用编号颜色：参考文献，公式，图，表，算法 等……
\refcolor{off}   % 开启: on[默认]; 关闭: off;
% 摘要和正文从右侧页开始
\beginright{off} % 开启: on[默认]; 关闭: off;
% 空白页留字
% \emptypagewords{[ -- This page is a preset empty page -- ]}

%=================================================================
% buaa模板已内嵌以下LaTeX工具包:
%--------------------
% ifthen, etoolbox, titletoc, remreset,
% geometry, fancyhdr, setspace,
% float, graphicx, subfigure, epstopdf,
% array, enumitem,
% booktabs, longtable, multirow, caption,
% listings, algorithm2e, amsmath, amsthm,
% hyperref, pifont, color, soul,
% ---
% For Win: times
% For Lin: newtxtext, newtxmath
% For Mac: times, fontspec
%--------------------
% 请在此处添加额外工具包>>


%=================================================================
% buaa模板已内嵌以下LaTeX宏:
%--------------------
% \highlight{text} % 黄色高亮
%--------------------
% 请在此处添加自定义宏>>


%%=================================================================
% 论文题目及副标题-{中文}{英文}
\Title{基于Transformer的知识图谱补全算法研究}{Research of Knowledge Graph Completion Algorithm Based on Transformer}
% \Subtitle{版本 \BUAAThesisVer{}}{Version \BUAAThesisVer{}}

% 学科大类,默认工学
% \Branch{工学}

% 院系,专业及研究方向
\Department{软件学院}
\Major{软件工程}
\Feild{软件工程}

% 导师信息-{中文名}{英文名}{职称}
\Tutor{谭火彬}{Tan Huobin}{副教授}
% \Cotutor{副导师姓名}{Cotutor}{高工}

% 学生姓名-{中文名}{英文名}
\Author{朱桐}{Zhu Tong}
% 学生学号
\StudentID{SY2121127}

% 中图分类号
\CLC{TP391.4}

% 时间节点-{月}{日}{年}
\DateEnroll{09}{01}{2021}
\DateGraduate{05}{16}{2024}
\DateSubmit{05}{10}{2024}
\DateDefence{05}{20}{2024}

%%=================================================================
% 摘要-{中文}{英文}
\Abstract{%
知识图谱在多个人工智能领域中得到了广泛的应用。但是由于现实世界不断发展带来的知识的动态变化，即使是大规模知识图谱也难以囊括所有的知识，影响了知识图谱在下游任务中的应用，因此知识图谱补全任务成为了热门的研究领域。知识图谱补全任务旨在自动化预测知识图谱中缺失的三元组，目前的主流方案是知识图谱嵌入，将实体和关系投影到低维向量空间中以学习知识图谱中的隐含规律。许多现有的方法通过利用图谱的图结构信息获得优异的性能，其中最具代表性的为利用图谱中局部邻域结构的基于图神经网络的方法。但是一方面图神经网络其浅层的网络结构限制了模型的表达能力，另一方面存在的过度平滑问题也导致模型无法利用长距离的信息。Transformer是注意力机制方面里程碑式的工作，基于Transformer的模型变体在计算机视觉和编程语言领域中表现出了出色的性能。因此针对上述不足，本文研究基于Transformer的知识图谱补全方法，首先提出了基于领域感知的Transformer模型(Neighborhood Aware Transformer for Link Prediction, NATLP)，提高了模型的表达能力获得了更好的知识图谱补全性能，并在此基础上进一步提出了结合图路径和局部邻域的Transformer模型 (A Transformer-based Knowledge Graph Embedding Model Combining
Graph Paths and Local Neighborhood，TKGE-PN)，实现了对于知识图谱中长短距离依赖的综合学习。本文的主要工作内容如下：

（1）针对基于图神经网络的知识图谱嵌入方法表达能力不足、无法捕捉邻居实体之间的相互依赖的问题，本文提出了基于领域感知的Transformer模型NATLP，通过关系特定的邻居实体信息构造建模了不同关系对于实体传递消息的影响，并对Transformer的自注意力计算机制进行了改造，使Transformer能够感知到局部邻域内的图结构数据，学习到邻居实体之间的相互依赖。

（2）针对基于图神经网络的方法以及NATLP无法学习到长距离依赖的问题，本文提出了结合图路径和局部领域的Transformer模型TKGE-PN。模型首先通过有偏随机游走算法以中心实体为起点采样多条图路径，其次通过基于Transformer的图路径编码模块学习图路径中的长距离依赖并转化为向量表示，最后局部邻域编码模块集合所有图路径向量结合局部邻域信息表示对三元组打分。在图路径编码的过程中，利用掩蔽实体关系预测任务增强模型对于长距离信息的学习能力。

论文在两个主流基准数据集FB15K-237和WN18RR上对提出的NATLP和TKGE-PN模型进行了实验，实验结果表明本文提出的两个模型在两个数据集上的多项指标超越了绝大部分基线模型，证明了本文提出的关键设计能够有效提高知识图谱补全任务的性能。
  }{Knowledge graphs have been extensively applied across multiple artificial intelligence domains. However, dynamic changes in knowledge due to the continuous development of the real world make it challenging for even large-scale knowledge graphs to encompass all information, affecting their application in downstream tasks. Consequently, the task of knowledge graph completion has emerged as a popular research area. Knowledge graph completion aims at automatically predicting missing triples within the knowledge graphs, and the mainstream solution currently is knowledge graph embedding, which projects entities and relations into a low-dimensional vector space to learn the implicit patterns within the knowledge graph. Many existing methods have achieved remarkable performance by utilizing the graph structure \\    information of the graph, with the most representative being methods based on graph neural networks that utilize local neighborhood structures within the graph. Nevertheless, the shallow network structure of graph neural networks limits the model's expressive power, and the existing over-smoothing problem also prevents the model from utilizing long-range information. Transformer represents a milestone in attention mechanisms, and Transformer-based model variants have demonstrated exceptional performance in the fields of computer vision and programming languages. In response to the aforementioned shortcomings, this paper investigates Transformer-based knowledge graph completion methods. We initially propose the Neighborhood Aware Transformer for Link Prediction (NATLP), which improves the model's expressive power to achieve better knowledge graph completion performance. Building on this, we further propose a Transformer-based Knowledge Graph Embedding Model Combining Graph Paths and Local Neighborhood (TKGE-PN), achieving comprehensive learning of both short and long-distance dependencies within the knowledge graph. The main contributions of this paper are as follows:

  (1) Addressing the issue of insufficient expressive capabilities of knowledge graph embedding methods based on graph neural networks and their inability to capture the interdependencies between neighbor entities, this paper proposes the NATLP. By constructing and modeling the influence of different relations on entity message passing using relationship-specific neighbor entity information, we modify the Transformer's self-attention mechanism to enable the Transformer to perceive the graph structure data within the local neighborhood, thereby learning the \\ interdependencies between neighbor entities.
  
  (2) To address the problem that graph neural network-based methods and NATLP cannot learn long-distance dependencies, this paper proposes TKGE-PN, a Transformer-based model combining graph paths and local neighborhood. The model first samples multiple graph paths starting from the central entity using a biased random walk algorithm. Next, it learns the long-distance dependencies within graph paths through a Transformer-based graph path encoding module, converting them into vector representations. Finally, a local neighborhood encoding module aggregates all graph path vectors with local neighborhood information to score triples. During graph path encoding, a masked entity-relation prediction task is utilized to enhance the model's ability to learn long-distance information.
  
  Experiments on two mainstream benchmark datasets, FB15K-237 and WN18RR, are conducted to evaluate the proposed NATLP and TKGE-PN models. The experimental results show that both models proposed in this paper surpass the majority of baseline models on several metrics on both datasets, proving that the key designs proposed in this paper can effectively improve the performance of knowledge graph completion tasks.



}
% 关键字-{中文}{英文}
\Keyword{%
    知识图谱，知识图谱补全，知识图谱嵌入，Transformer，图路径，局部邻域
  }{%
    Knowledge Graph, Knowledge Graph Completion,
Knowledge Graph Embedding, Transformer, Graph Path, Local Neighborhood

}
% Given the widespread application of Knowledge Graphs (KGs) across multiple Artificial Intelligence domains, the incompleteness of most current knowledge graphs has made Knowledge Graph Completion (KGC) a hot research area. KGC aims to predict missing triples in a knowledge graph, and the current mainstream solution is Knowledge Graph Embeddings (KGE), which projects entities and relations into a low-dimensional vector space to learn the hidden patterns within the knowledge graph. Many existing methods achieve excellent performance by utilizing the graph structure information of the graph, among which the most representative is the method based on Graph Neural Networks (GNNs) that utilizes the local neighborhood structure in the graph. However, the shallow network structure of GNNs limits the model's expressive power on one hand, and on the other hand, the issue of over-smoothing also prevents the model from utilizing long-distance information. Transformer, a milestone work in the field of attention mechanisms, and its variants have shown outstanding performance in the fields of Computer Vision and Programming Languages. Therefore, to address the aforementioned deficiencies, this paper investigates Transformer-based methods for Knowledge Graph Completion, introducing a domain-aware Transformer model, NATLP, and a Transformer model that combines graph paths and local neighborhoods, TKGE-PN. The main contributions of this paper are as follows:

% (1) To address the issue of limited expressive power and inability to capture mutual dependencies among neighboring entities in knowledge graph embedding methods based on GNNs, this paper proposes NATLP, a neighborhood aware transformer for Link Prediction that constructs and models the impact of different relations on entity message passing based on relation-specific neighboring entity information. The Transformer's self-attention mechanism is also modified to enable it to perceive local neighborhood graph structure data and learn mutual dependencies among neighboring entities.

% (2) To address the issue that methods based on Graph Neural Networks, as well as NATLP, fail to learn long-distance dependencies, this paper introduces a transformer-based knowledge graph embedding Model combining Graph Paths and Local Neighborhood, TKGE-PN. The model first samples multiple graph paths centered on the target entity using a biased random walk algorithm. Then, it employs a Transformer-based graph path encoding module to learn and vectorize the long-distance dependencies within these paths. Lastly, a local neighborhood encoding module aggregates all the graph path vectors along with local neighborhood information to score triples. During the graph path encoding process, the model's ability to learn long-distance information is enhanced by employing a masked entity relation prediction task.

% (3) Experiments on two mainstream benchmark datasets, FB15K-237 and WN18RR, demonstrated the effectiveness of the proposed NATLP and TKGE-PN models along with the key designs.



% 图标目录
\Listfigtab{on} % 启用: on[默认]; 双标题：bi; 关闭: off;

% 缩写定义 按tabular环境或其他列表环境编写
% \Abbreviations{ \centering
% \begin{tabular}{cl}
%   $E$ & 能量 \\
%   $m$ & 质量 \\
%   $c$ & 光速 \\
%   $P$ & 概率 \\
%   $T$ & 时间 \\
%   $v$ & 速度 \\
% \end{tabular}
% }

\begin{document}

%%=================================================================
% 标题级别
%--------------------
% \chapter{第一章}
% \section{1.1 小节}
% \subsection{1.1.1 条}
% \subsubsection{1.1.1.1}
% \paragraph{1.1.1.1.1}
% \subparagraph{1.1.1.1.1.1}
%--------------------
%%=================================================================

% 绪论
\input{tex/chap_introduction}

\input{tex/chap_theoreticalbasis}

\input{tex/chap_NATLP}

\input{tex/chap_experiment}

\input{tex/chap_TKGE-PN}

\input{tex/chap_TKGE-PN_experiment.tex}

% 总结
\input{tex/chap_summary}

% 参考文献
% 手册中参考文献标准似乎并没有严格按照国标GBT7714-2015执行
\Bib{bst/GBT7714-BUAA}{ref}

% 攻读学位期间成果
\input{tex/chap_achievement}

% 致谢
\input{tex/chap_acknowledge}

\vspace{5cm}

\end{document}
