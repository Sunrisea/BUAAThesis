\chapter{基于邻域感知的Transformer模型}

% Transformer网络虽然在自然语言处理（NLP）领域取得了巨大成功，但在直接应用到图表示学习（Graph Representation Learning）领域时会遇到一系列挑战和困难。图表示学习涉及到处理图数据，图数据具有非欧几里得结构，这与NLP中处理的序列数据（一维结构）有本质区别。下面列举了一些Transformer在图表示学习中面临的主要困难：

% 1. **非序列结构**：图是一种无序的数据结构，节点之间的关系是通过边来定义的，并且没有固定的顺序。相比之下，Transformer网络原本是为处理序列数据设计的，它使用位置编码来保留序列中元素的顺序信息。在图中，节点没有自然的顺序，因此Transformer网络无法直接使用位置编码来捕捉节点间的结构关系。

% 2. **邻居多样性**：在图中，每个节点可能有不同数量的邻居。这种动态的、不规则的邻接关系与Transformer网络中假设的固定大小的输入上下文不相符。因此，直接应用自注意力机制来学习图节点表示时难以处理不同节点的可变邻居数量。

% 3. **长距离依赖**：尽管Transformer网络擅长捕捉长距离依赖，但在图中，这种依赖通常是通过节点之间的路径来定义的，而不是序列中的线性位置。因此，Transformer需要被修改或扩展，以便能够理解和利用图中的路径和结构模式。

% 4. **缩放问题**：对于大型图，使用Transformer进行全图的自注意力学习是计算资源密集型的，因为复杂度是图中节点数量的平方。这使得直接在大规模图上应用Transformer变得不切实际。

% 5. **边的信息和类型**：在图中，边可能包含重要的信息，并且边的类型（如社交网络中的不同关系类型）也可能对图表示至关重要。然而，标准的Transformer模型并没有直接的方式来编码和使用边的信息。

% 为了克服这些困难，研究人员已经提出了许多不同的方法和模型的变种，如图注意力网络（Graph Attention Networks, GATs）和图Transformer网络（Graph Transformer Networks），这些模型专门设计或修改以适应图数据的特性。这些变种通常包括对自注意力机制的修改，以更好地处理图的结构性质，以及引入适合于图结构的新型位置编码或相对位置编码。此外，为了处理大型图，研究人员还在探索如何有效地对图进行采样，以及如何利用稀疏性和分层结构来降低计算复杂度。

% Transformer在知识图谱嵌入任务中具有以下优点：

% 1. 处理长距离依赖关系：知识图谱中的实体和关系之间存在着复杂的长距离依赖关系。传统的基于图的模型（如Graph Convolutional Networks）往往只考虑局部邻居信息，难以捕捉到远距离的语义依赖。而Transformer通过自注意力机制可以处理全局上的依赖关系，能够更好地捕捉实体和关系之间的长距离语义关联。

% 2. 并行计算能力：与循环神经网络不同，Transformer网络可以进行并行计算，加快了训练和推理的速度。在大规模的知识图谱中，由于实体和关系的数量庞大，传统的基于图的模型可能面临计算效率低下的问题，而Transformer网络能够更高效地处理大规模的知识图谱数据。

% 3. 对复杂属性建模：知识图谱中的实体通常具有丰富的属性信息，如文本描述、分类标签等。传统的基于图的模型往往无法有效地捕捉和利用这些属性信息，而Transformer网络可以将这些属性信息作为输入，并通过多头注意力机制对不同属性进行建模，从而更好地捕捉实体的语义特征。

% 4. 高度可扩展性：Transformer网络可以通过增加层数和调整参数来提高模型的表示能力。在知识图谱嵌入任务中，复杂的语义关系和多层次的结构需要一个具有较强建模能力的模型来学习。Transformer网络的多层结构可以提供更强的表示能力，可以通过增加层数来适应不同任务的需求。

% 综上所述，Transformer在知识图谱嵌入任务中具有处理长距离依赖关系、并行计算能力、对复杂属性建模和高度可扩展性等优点，为知识图谱的表示学习提供了一种强大而灵活的模型选择。

%注意力中添加度数：1-1/度数乘积

\section{现有问题描述和分析}

图卷积神经网络GCN于2017年被提出，对原先图神经网络中基于谱空间的的图卷积算子进行了优化，降低了模型的复杂度，由此引发了图神经网络的研究热潮。图神经网络迅速成为了图结构数据处理的重要方式，在社交网络\upcite{social}、推荐系统、知识图谱等多个领域都有着重要应用。近几年，图神经网络的应用是知识图谱嵌入领域非常重要的进展。图卷积神经网络能够直接处理图结构数据并捕捉知识图谱中的拓扑结构，通过聚合邻居节点的信息，图神经网络可以有效地学习知识图谱中节点（实体）和边（关系的嵌入表示）。相较于之前的方法，基于图神经网络的知识图谱嵌入方法获得了很大的性能提升。

然而，受限于本身的网络结构，在进行知识图谱嵌入时，基于图神经网络的方法依然存在不足，导致其性能受限。首先，图卷积神经网络采用聚合邻居节点的方法来更新中心实体的表示，在这个过程中，各个邻居节点传递的信息之间是互不感知、互相独立的，这样的聚合方式没有将邻居节点信息之间的相互依赖纳入考虑；其次，图神经网络采用的消息传递模式整体模型结构比较简单，使模型的表达能力受到了限制，在挖掘图谱中实体和实体、实体与关系之间的复杂交互上存在困难。

而在以上两个方面，Transformer网络存在巨大的优势。首先，通过构造查询向量、键向量和值向量来进行注意力的计算以及采用多头注意力机制，相比于图神经网络，Transformer能够更加高效地挖掘输入之间各个维度的复杂交互；同时，通过调整模型的层数、头的数量或是隐层的维度大小，Transformer可以很容易地适应处理不同规模和复杂度的知识图谱的需求。

其次，Transformer网络的自注意力机制能够有效地捕捉序列中任意两个元素之间的全局依赖关系。在知识图谱嵌入的场景之中，这意味着模型在挖掘局部邻域的结构信息时，除了邻居节点和中心节点之间的依赖之外，还能够同时学习到邻居节点之间的长距离依赖，捕获邻居节点传递的信息之间的相互影响。此外，Transformer架构还支持模型的预训练和迁移学习，可以首先在一个大规模的综合性知识图谱上进行预训练，然后迁移到特定领域的知识图谱上，通过这样的方式，可以减少模型对标注数据的依赖，提高模型在特定任务上的表现。

但是，虽然Transformer网络在自然语言处理（NLP）领域已经取得了巨大成功，但将Transformer网络直接应用到知识图谱嵌入领域时依旧会遇到一系列挑战和困难。图具有非欧几里得结构，是一种无序的数据结构，这与NLP中处理的序列数据（一维结构）有本质区别。Transformer网络原本是为处理序列数据设计的，它使用位置编码来保留序列中元素的顺序信息。但是，在知识图谱中，节点之间的关系是通过边来定义的，并且没有固定的顺序，因此Transformer网络无法直接使用位置编码来捕捉节点间的结构关系。部分方法例如MAGNN\upcite{MAGNN}选择通过随机游走的方式来将图数据转化为序列数据来处理，但这样的方式会导致图结构信息的失真。

此外，在知识图谱中，边，即关系，反映着实体和实体之间不同的交互方式，蕴含着丰富的语义信息。两个实体之间连接的关系不同，传递的信息可能是千差万别的，因此如何采用合适的方式来对利用关系信息，挖掘关系对于消息传递的影响十分重要。但标准的Transformer模型并没有直接的方式来编码和使用边的信息。部分利用Transformer来进行知识图谱表示学习的模型例如Relphormer\upcite{Relphormer}将知识图谱中的实体和关系视为地位相同的节点，采用同样的方式进行处理，这样的方式虽然解决了边的表示问题，但没有考虑到知识图谱中实体和关系的差异性。

为了解决上述问题，本文提出了一种基于邻域感知的Transformer模型用于链路预测任务(Neighborhood Aware Transformer for Link Prediction, NATLP)。首先，在模型输入信息构造阶段，为了充分建模不同关系对于实体传递消息的影响，模型引入了超网络基于关系生成特定网络参数，完成关系特定的邻居实体信息构造。其次，为了让Transformer能够更好地处理图结构数据，模型对Transformer的自注意力机制进行了改造，提出了一种融合图结构的自注意力机制，使得Transformer能够学习到输入消息之间的互相依赖。

\section{NATLP模型设计}

\subsection{符号定义}

为了能够清晰地说明论文提出地NATLP模型的细节

\subsection{模型总体结构}

\subsection{关系特定的邻居实体消息构造}

链路预测任务的目标是利用知识图谱中已有的事实去预测未知事实在知识图谱中的存在概率，因此在完成数据的预处理之后，模型的下一步任务是将是将已有的事实转化为模型能够接受的形式作为输入。为了帮助预测三元组中缺失的尾实体，模型需要首先获得局部邻域中邻居实体向中心实体传递的信息。而知识图谱中的关系反映着实体和实体之间不同的交互方式，对实体之间传递的信息有着很大的影响。因此，参考图神经网络的消息传递模型，NATKG首先会基于邻居实体和连接的关系，完成邻居实体的消息构造。

