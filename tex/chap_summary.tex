% !TeX root = ../Template.tex
% 总结
\summary
自从2017年被提出，Transformer已经在计算机视觉、自然语言处理等多个领域大放异彩，展现出了巨大的潜力，收到了许多的关注，因此也有不少工作尝试将Transformer网络引入到知识图谱补全领域中。本课题针对传统知识图谱嵌入方法和基于图神经网络的方法的缺点，研究如何利用Transformer模型来学习知识图谱中的语义和结构信息，提升知识图谱补全任务的性能，提出了两种新型的基于Transformer的知识图谱补全方法：基于邻域感知的Transformer模型NATLP和结合图路径和局部邻域的Transformer模型TKGE-PN。

NATLP针对基于图神经网络的知识图谱嵌入方法表达能力不足，Transformer无法直接学习图结构的问题，对Transformer的自注意力机制进行了改造，提出了一种融合图结构的自注意力机制，使得模型能够学习到输入消息之间的互相依赖；
针对关系和实体间的交互建模不足的问题，NATLP在模型输入信息构造阶段，基于关系生成特定的网络参数，实现关系特定的邻居信息构造,建模不同关系对于实体传递消息的影响.

TKGE-PN通过对知识图谱中的局部邻域和图路径两种结构信息的融合，完成了在利用丰富的邻域信息的同时对于长距离信息的挖掘，并利用Transformer模型强大的表达能力实现了对于图谱中长短距离依赖的综合学习。在两个标准数据集WN18RR和FB15k-237上，TKGE-PN链路预测任务的性能表现超越绝大多数现有的嵌入模型，证明了方法的有效性。

未来，我们计划进一步探索对于知识图谱中结构信息的利用，包括更有效率的利用方式和更加丰富的结构信息种类，同时尝试将TKGE-PN应用到除链路预测之外的其他知识图谱表示学习任务中。此外，TKGE-PN基于Transformer的结构带来的大量资源开销如何进行优化也是一个值得研究的方向。

