% !TeX root = ../Template.tex
% 总结
\summary
自从2017年被提出，Transformer已经在计算机视觉、自然语言处理等多个领域大放异彩，展现出了巨大的潜力，收到了许多的关注，因此也有不少工作尝试将Transformer网络引入到知识图谱补全领域中。本课题针对传统知识图谱嵌入方法和基于图神经网络的方法的缺点，研究如何利用Transformer模型来学习知识图谱中的语义和结构信息，提升知识图谱补全任务的性能，提出了两种新型的基于Transformer的知识图谱补全方法：基于邻域感知的Transformer模型NATLP和结合图路径和局部邻域的Transformer模型TKGE-PN。

针对基于图神经网络的知识图谱嵌入方法表达能力不足的问题，NATLP研究利用Transformer网络来完成知识图谱补全。针对Transformer无法直接学习图结构的问题等问题，NALTP对Transformer的自注意力机制进行了改造，提出了一种融合图结构的自注意力机制，使得模型能够学习到输入消息之间的互相依赖；针对关系和实体间的交互建模不足的问题，NATLP在模型输入信息构造阶段，基于关系生成特定的网络参数，实现关系特定的邻居信息构造，建模不同关系对于实体传递消息的影响。

针对NATLP模型和基于图神经网络的方法缺乏捕捉图谱中长距离依赖的缺点，TKGE-PN通过对知识图谱中的局部邻域和图路径两种结构信息的融合，完成了在利用丰富的邻域信息的同时对于图谱中长距离信息的挖掘，提高了知识图谱补全任务的性能。TKGE-PN首先通过基于有偏随机游走的采样算法对图路径进行采样，随后通过基于Transformer的图路径编码模块学习其中的长距离依赖，并通过掩蔽实体关系任务实现长短距离信息的平衡；最后通过局部邻域编码模块实现了图路径和局部邻域结构信息的综合应用。在两个标准数据集WN18RR和FB15k-237上，TKGE-PN链路预测任务的性能表现超越绝大多数现有的嵌入模型，证明了方法的有效性。

未来，我们计划进一步探索对于知识图谱中结构信息的利用，包括更有效率的利用方式和更加丰富的结构信息种类，同时尝试将TKGE-PN应用到除链路预测之外的其他知识图谱表示学习任务中。此外，TKGE-PN基于Transformer的结构带来的大量资源开销如何进行优化也是一个值得研究的方向。

