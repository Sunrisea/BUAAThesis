% !TeX root = ../Template.tex
% 总结
\summary
% 自从2017年被提出，Transformer已经在计算机视觉、自然语言处理等多个领域大放异彩，展现出了巨大的潜力，收到了许多的关注，因此也有不少工作尝试将Transformer网络引入到知识图谱补全领域中。
本课题针对传统知识图谱嵌入方法和基于图神经网络的方法的缺点，研究如何利用Transformer模型来学习知识图谱中的语义和结构信息，提升知识图谱补全任务的性能，提出了两种新型的基于Transformer的知识图谱补全方法：基于邻域感知的Transformer模型NATLP，并在NATLP模型的基础上进一步提出了结合图路径和局部邻域的Transformer模型TKGE-PN。

针对基于图神经网络的知识图谱嵌入方法表达能力不足的问题，NATLP研究利用Transformer网络来完成知识图谱补全。针对以往模型对关系和实体间的交互建模不足的问题，NATLP在模型输入信息构造阶段，基于关系生成特定的网络参数，实现关系特定的邻居信息构造，显示建模了不同关系对于实体传递消息的影响；针对Transformer无法直接感知图结构的问题，NALTP对Transformer的自注意力机制进行了改造，提出了一种融合图结构的自注意力机制，使得模型能够学习到输入消息之间的互相依赖；

NATLP模型解决了基于图神经网络的模型存在的部分问题，但依然存在无法有效学习长距离依赖的缺陷。针对基于图神经网络的方法和NATLP方法中的以上缺陷，本文在NATLP的基础上进一步提出了结合图路径和局部邻域的Transformer模型TKGE-PN。TKGE-PN通过对知识图谱中的局部邻域和图路径两种结构信息的融合，完成了在利用丰富的邻域信息的同时对于图谱中长距离信息的挖掘，提高了知识图谱补全任务的性能。TKGE-PN首先通过基于有偏随机游走的采样算法对图路径进行采样，随后通过基于Transformer的图路径编码模块学习其中的长距离依赖，并通过掩蔽实体关系任务实现长短距离信息的平衡；最后通过局部邻域编码模块实现了图路径和局部邻域结构信息的综合应用。

实验结果表明，在两个标准数据集WN18RR和FB15k-237上，NATLP和TKGE-PN的链路预测任务的性能表现超越绝大多数现有的嵌入模型，证明了本文提出的两个模型以及其中的关键设计的有效性。

% 未来，我们计划进一步探索对于知识图谱中结构信息的利用，包括更有效率的利用方式和更加丰富的结构信息种类，同时尝试将TKGE-PN应用到除链路预测之外的其他知识图谱表示学习任务中。

本文涉及的研究内容还有许多可以拓展的地方。首先，知识图谱的图结构除了图路径和局部邻域之外还有其他的表现形式，我们计划进一步探索对于知识图谱中结构信息的利用，例如在规模更大的子图范围上来应用Transformer网络，进一步挖掘模型的潜力。

其次，除了结构信息之外，知识图谱中也蕴含着丰富的文本信息，例如实体的文字描述。而Transformer网络被广泛应用于预训练语言模型中，在挖掘文本语义信息方面存在天然优势。未来我们计划研究将模型与预训练语言模型进行结合，同时利用文本信息和图结构信息来完成知识图谱补全任务。

最后，Transformer网络的复杂度较高，训练和预测时的开销较大，在处理大规模知识图谱时可能会遇到资源上的瓶颈。因此如何优化基于Transformer的结构带来的大量资源开销也是一个值得研究的方向。

